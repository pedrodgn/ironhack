
<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Dimensionality reduction - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"6a2a29ac-a4f7-4f2d-a43f-0465cc80524a","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Dimensionality_reduction","wgTitle":"Dimensionality reduction","wgCurRevisionId":965821499,"wgRevisionId":965821499,"wgArticleId":579867,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Articles with short description","All articles with unsourced statements","Articles with unsourced statements from September 2017","Wikipedia articles needing clarification from September 2017","Articles with unsourced statements from June 2017","Dimension reduction",
"Machine learning"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Dimensionality_reduction","wgRelevantArticleId":579867,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgWikibaseItemId":"Q16000077","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0,"wgULSPosition":"interlanguage"};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":
"ready","user.options":"loading","ext.cite.styles":"ready","skins.vector.styles.legacy":"ready","mediawiki.toc.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","site","mediawiki.page.startup","mediawiki.page.ready","mediawiki.toc","skins.vector.legacy.js","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.extra-toolbar-buttons","ext.gadget.refToolbar","ext.gadget.switcher","ext.centralauth.centralautologin","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.toc.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.35.0-wmf.39"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Dimensionality_reduction&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=Dimensionality_reduction&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Dimensionality_reduction"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 9]><script src="/w/resources/lib/html5shiv/html5shiv.js"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Dimensionality_reduction rootpage-Dimensionality_reduction skin-vector action-view skin-vector-legacy minerva--history-page-action-enabled">
<div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
		<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div>
	<div class="mw-indicators mw-body-content">
	</div>
	<h1 id="firstHeading" class="firstHeading" lang="en">Dimensionality reduction</h1>
	
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
		<div id="contentSub"></div>
		
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#searchInput">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Process of reducing the number of random variables under consideration</div>
<div role="note" class="hatnote navigation-not-searchable">For dimensional reduction in physics, see <a href="/wiki/Dimensional_reduction" title="Dimensional reduction">Dimensional reduction</a>.</div>
<p><b>Dimensionality reduction</b>, or <b>dimension reduction</b>, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its <a href="/wiki/Intrinsic_dimension" title="Intrinsic dimension">intrinsic dimension</a>. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often <a href="/wiki/Sparse_matrix" title="Sparse matrix">sparse</a> as a consequence of the <a href="/wiki/Curse_of_dimensionality" title="Curse of dimensionality">curse of dimensionality</a>, and analyzing the data is usually <a href="/wiki/Computational_complexity_theory#Intractability" title="Computational complexity theory">computationally intractable</a>. Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as <a href="/wiki/Signal_processing" title="Signal processing">signal processing</a>, <a href="/wiki/Speech_recognition" title="Speech recognition">speech recognition</a>, <a href="/wiki/Neuroinformatics" title="Neuroinformatics">neuroinformatics</a>, and <a href="/wiki/Bioinformatics" title="Bioinformatics">bioinformatics</a>.<sup id="cite_ref-dr_review_1-0" class="reference"><a href="#cite_note-dr_review-1">&#91;1&#93;</a></sup>
</p><p>Methods are commonly divided into linear and non-linear approaches.<sup id="cite_ref-dr_review_1-1" class="reference"><a href="#cite_note-dr_review-1">&#91;1&#93;</a></sup> Approaches can also be divided into <a href="/wiki/Feature_selection" title="Feature selection">feature selection</a> and <a href="/wiki/Feature_extraction" title="Feature extraction">feature extraction</a>.<sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup> Dimensionality reduction can be used for <a href="/wiki/Noise_reduction" title="Noise reduction">noise reduction</a>, <a href="/wiki/Data_visualization" title="Data visualization">data visualization</a>, <a href="/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a>, or as an intermediate step to facilitate other analyses. 
</p>
<div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Feature_selection"><span class="tocnumber">1</span> <span class="toctext">Feature selection</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Feature_projection"><span class="tocnumber">2</span> <span class="toctext">Feature projection</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Principal_component_analysis_(PCA)"><span class="tocnumber">2.1</span> <span class="toctext">Principal component analysis (PCA)</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Non-negative_matrix_factorization_(NMF)"><span class="tocnumber">2.2</span> <span class="toctext">Non-negative matrix factorization (NMF)</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="#Kernel_PCA"><span class="tocnumber">2.3</span> <span class="toctext">Kernel PCA</span></a></li>
<li class="toclevel-2 tocsection-6"><a href="#Graph-based_kernel_PCA"><span class="tocnumber">2.4</span> <span class="toctext">Graph-based kernel PCA</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="#Linear_discriminant_analysis_(LDA)"><span class="tocnumber">2.5</span> <span class="toctext">Linear discriminant analysis (LDA)</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#Generalized_discriminant_analysis_(GDA)"><span class="tocnumber">2.6</span> <span class="toctext">Generalized discriminant analysis (GDA)</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Autoencoder"><span class="tocnumber">2.7</span> <span class="toctext">Autoencoder</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="#t-SNE"><span class="tocnumber">2.8</span> <span class="toctext">t-SNE</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="#UMAP"><span class="tocnumber">2.9</span> <span class="toctext">UMAP</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-12"><a href="#Dimension_reduction"><span class="tocnumber">3</span> <span class="toctext">Dimension reduction</span></a></li>
<li class="toclevel-1 tocsection-13"><a href="#Applications"><span class="tocnumber">4</span> <span class="toctext">Applications</span></a></li>
<li class="toclevel-1 tocsection-14"><a href="#See_also"><span class="tocnumber">5</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-15"><a href="#Notes"><span class="tocnumber">6</span> <span class="toctext">Notes</span></a></li>
<li class="toclevel-1 tocsection-16"><a href="#References"><span class="tocnumber">7</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-17"><a href="#External_links"><span class="tocnumber">8</span> <span class="toctext">External links</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Feature_selection">Feature selection</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=1" title="Edit section: Feature selection">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Feature_selection" title="Feature selection">Feature selection</a></div><div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/Combinatorial_optimization" title="Combinatorial optimization">Combinatorial optimization</a></div>
<p><a href="/wiki/Feature_selection" title="Feature selection">Feature selection</a> approaches try to find a subset of the input variables (also called features or attributes). The three strategies are: the <i>filter</i> strategy (e.g. <a href="/wiki/Information_gain_in_decision_trees" title="Information gain in decision trees">information gain</a>), the <i>wrapper</i> strategy (e.g. search guided by accuracy), and the <i>embedded</i> strategy (selected features add or are removed while building the model based on prediction errors).
</p><p><a href="/wiki/Data_analysis" title="Data analysis">Data analysis</a> such as <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a> or <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> can be done in the reduced space more accurately than in the original space.<sup id="cite_ref-3" class="reference"><a href="#cite_note-3">&#91;3&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Feature_projection">Feature projection</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=2" title="Edit section: Feature projection">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Feature_extraction" title="Feature extraction">Feature extraction</a></div>
<p>Feature projection (also called Feature extraction) transforms the data from the <a href="/wiki/High-dimensional_space" class="mw-redirect" title="High-dimensional space">high-dimensional space</a> to a space of fewer dimensions. The data transformation may be linear, as in <a href="/wiki/Principal_component_analysis" title="Principal component analysis">principal component analysis</a> (PCA), but many <a href="/wiki/Nonlinear_dimensionality_reduction" title="Nonlinear dimensionality reduction">nonlinear dimensionality reduction</a> techniques also exist.<sup id="cite_ref-4" class="reference"><a href="#cite_note-4">&#91;4&#93;</a></sup><sup id="cite_ref-5" class="reference"><a href="#cite_note-5">&#91;5&#93;</a></sup> For multidimensional data, <a href="/wiki/Tensor" title="Tensor">tensor</a> representation can be used in dimensionality reduction through <a href="/wiki/Multilinear_subspace_learning" title="Multilinear subspace learning">multilinear subspace learning</a>.<sup id="cite_ref-MSLsurvey_6-0" class="reference"><a href="#cite_note-MSLsurvey-6">&#91;6&#93;</a></sup>
</p>
<h3><span id="Principal_component_analysis_.28PCA.29"></span><span class="mw-headline" id="Principal_component_analysis_(PCA)">Principal component analysis (PCA)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=3" title="Edit section: Principal component analysis (PCA)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Principal_component_analysis" title="Principal component analysis">Principal component analysis</a></div>
<p>The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the <a href="/wiki/Covariance" title="Covariance">covariance</a> (and sometimes the <a href="/wiki/Correlation_and_dependence" title="Correlation and dependence">correlation</a>) <a href="/wiki/Matrix_(mathematics)" title="Matrix (mathematics)">matrix</a> of the data is constructed and the <a href="/wiki/Eigenvalue,_eigenvector_and_eigenspace" class="mw-redirect" title="Eigenvalue, eigenvector and eigenspace">eigenvectors</a> on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behavior of the system, because they often contribute the vast majority of the system's energy, especially in low-dimensional systems. Still, this must be proven on a case-by-case basis as not all systems exhibit this behavior.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (September 2017)">citation needed</span></a></i>&#93;</sup><sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="The reason for this is unclear. (September 2017)">why?</span></a></i>&#93;</sup>. The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors.
</p>
<h3><span id="Non-negative_matrix_factorization_.28NMF.29"></span><span class="mw-headline" id="Non-negative_matrix_factorization_(NMF)">Non-negative matrix factorization (NMF)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=4" title="Edit section: Non-negative matrix factorization (NMF)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">Non-negative matrix factorization</a></div>
<p>NMF decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist.<sup id="cite_ref-lee-seung_7-0" class="reference"><a href="#cite_note-lee-seung-7">&#91;7&#93;</a></sup><sup id="cite_ref-lee2001algorithms_8-0" class="reference"><a href="#cite_note-lee2001algorithms-8">&#91;8&#93;</a></sup> such as astronomy<sup id="cite_ref-blantonRoweis07_9-0" class="reference"><a href="#cite_note-blantonRoweis07-9">&#91;9&#93;</a></sup><sup id="cite_ref-ren18_10-0" class="reference"><a href="#cite_note-ren18-10">&#91;10&#93;</a></sup>. NMF is well known since the multiplicative update rule by Lee &amp; Seung<sup id="cite_ref-lee-seung_7-1" class="reference"><a href="#cite_note-lee-seung-7">&#91;7&#93;</a></sup>, which has been continuously developed: the inclusion of uncertainties <sup id="cite_ref-blantonRoweis07_9-1" class="reference"><a href="#cite_note-blantonRoweis07-9">&#91;9&#93;</a></sup>, the consideration of missing data and parallel computation <sup id="cite_ref-zhu16_11-0" class="reference"><a href="#cite_note-zhu16-11">&#91;11&#93;</a></sup>, sequential construction<sup id="cite_ref-zhu16_11-1" class="reference"><a href="#cite_note-zhu16-11">&#91;11&#93;</a></sup> which leads to the stability and linearity of NMF<sup id="cite_ref-ren18_10-1" class="reference"><a href="#cite_note-ren18-10">&#91;10&#93;</a></sup>, as well as other <a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">updates</a> including handling missing data in <a href="/wiki/Digital_image_processing" title="Digital image processing">digital image processing</a>.<sup id="cite_ref-ren20”_12-0" class="reference"><a href="#cite_note-ren20”-12">&#91;12&#93;</a></sup>
</p><p>With a stable component basis during construction, and a linear modeling process, <a href="/wiki/Non-negative_matrix_factorization#Sequential_NMF" title="Non-negative matrix factorization">sequential NMF</a><sup id="cite_ref-zhu16_11-2" class="reference"><a href="#cite_note-zhu16-11">&#91;11&#93;</a></sup> is able to preserve the flux in direct imaging of circumstellar structures in astromony<sup id="cite_ref-ren18_10-2" class="reference"><a href="#cite_note-ren18-10">&#91;10&#93;</a></sup>, as one of the <a href="/wiki/Methods_of_detecting_exoplanets" title="Methods of detecting exoplanets">methods of detecting exoplanets</a>, especially for the direct imaging of <a href="/wiki/Circumstellar_disks" class="mw-redirect" title="Circumstellar disks">circumstellar disks</a>. In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA as demonstrated by Ren et al<sup id="cite_ref-ren18_10-3" class="reference"><a href="#cite_note-ren18-10">&#91;10&#93;</a></sup>.
</p>
<h3><span class="mw-headline" id="Kernel_PCA">Kernel PCA</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=5" title="Edit section: Kernel PCA">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Kernel_PCA" class="mw-redirect" title="Kernel PCA">Kernel PCA</a></div>
<p>Principal component analysis can be employed in a nonlinear way by means of the <a href="/wiki/Kernel_trick" class="mw-redirect" title="Kernel trick">kernel trick</a>. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is entitled <a href="/wiki/Kernel_PCA" class="mw-redirect" title="Kernel PCA">kernel PCA</a>.
</p>
<h3><span class="mw-headline" id="Graph-based_kernel_PCA">Graph-based kernel PCA</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=6" title="Edit section: Graph-based kernel PCA">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Other prominent nonlinear techniques include <a href="/wiki/Manifold_learning" class="mw-redirect" title="Manifold learning">manifold learning</a> techniques such as <a href="/wiki/Isomap" title="Isomap">Isomap</a>, <a href="/wiki/Locally_linear_embedding" class="mw-redirect" title="Locally linear embedding">locally linear embedding</a> (LLE),<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">&#91;13&#93;</a></sup> Hessian LLE, Laplacian eigenmaps, and methods based on tangent space analysis<sup id="cite_ref-14" class="reference"><a href="#cite_note-14">&#91;14&#93;</a></sup><sup id="cite_ref-15" class="reference"><a href="#cite_note-15">&#91;15&#93;</a></sup>. These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data, and can be viewed as defining a graph-based kernel for Kernel PCA.
</p><p>More recently, techniques have been proposed that, instead of defining a fixed kernel, try to learn the kernel using <a href="/wiki/Semidefinite_programming" title="Semidefinite programming">semidefinite programming</a>. The most prominent example of such a technique is <a href="/wiki/Maximum_variance_unfolding" class="mw-redirect" title="Maximum variance unfolding">maximum variance unfolding</a> (MVU). The central idea of MVU is to exactly preserve all pairwise distances between nearest neighbors (in the inner product space), while maximizing the distances between points that are not nearest neighbors.
</p><p>An alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces. Important examples of such techniques include: classical <a href="/wiki/Multidimensional_scaling" title="Multidimensional scaling">multidimensional scaling</a>, which is identical to PCA; <a href="/wiki/Isomap" title="Isomap">Isomap</a>, which uses geodesic distances in the data space; <a href="/wiki/Diffusion_map" title="Diffusion map">diffusion maps</a>, which use diffusion distances in the data space; <a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-distributed stochastic neighbor embedding</a> (t-SNE), which minimizes the divergence between distributions over pairs of points; and curvilinear component analysis.
</p><p>A different approach to nonlinear dimensionality reduction is through the use of <a href="/wiki/Autoencoder" title="Autoencoder">autoencoders</a>, a special kind of feed-forward <a href="/wiki/Neural_network" title="Neural network">neural networks</a> with a bottle-neck hidden layer.<sup id="cite_ref-16" class="reference"><a href="#cite_note-16">&#91;16&#93;</a></sup> The training of deep encoders is typically performed using a greedy layer-wise pre-training (e.g., using a stack of <a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">restricted Boltzmann machines</a>) that is followed by a finetuning stage based on <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a>.
</p>
<h3><span id="Linear_discriminant_analysis_.28LDA.29"></span><span class="mw-headline" id="Linear_discriminant_analysis_(LDA)">Linear discriminant analysis (LDA)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=7" title="Edit section: Linear discriminant analysis (LDA)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">Linear discriminant analysis</a></div>
<p>Linear discriminant analysis (LDA) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events.
</p>
<h3><span id="Generalized_discriminant_analysis_.28GDA.29"></span><span class="mw-headline" id="Generalized_discriminant_analysis_(GDA)">Generalized discriminant analysis (GDA)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=8" title="Edit section: Generalized discriminant analysis (GDA)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>GDA deals with nonlinear discriminant analysis using kernel function operator. The underlying theory is close to the <a href="/wiki/Support_vector_machine" title="Support vector machine">support vector machines</a> (SVM) insofar as the GDA method provides a mapping of the input vectors into high-dimensional feature space.<sup id="cite_ref-gda_17-0" class="reference"><a href="#cite_note-gda-17">&#91;17&#93;</a></sup><sup id="cite_ref-cloudid_18-0" class="reference"><a href="#cite_note-cloudid-18">&#91;18&#93;</a></sup> Similar to LDA, the objective of GDA is to find a projection for the features into a lower dimensional space by maximizing the ratio of between-class scatter to within-class scatter.
</p>
<h3><span class="mw-headline" id="Autoencoder">Autoencoder</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=9" title="Edit section: Autoencoder">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></div>
<p>Autoencoders can be used to learn non-linear dimension reduction functions and codings together with an inverse function from the coding to the original representation.
</p>
<h3><span class="mw-headline" id="t-SNE">t-SNE</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=10" title="Edit section: t-SNE">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/TSNE" class="mw-redirect" title="TSNE">tSNE</a></div>
<p>T-distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear dimensionality reduction technique useful for visualization of high-dimensional datasets.
</p>
<h3><span class="mw-headline" id="UMAP">UMAP</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=11" title="Edit section: UMAP">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Uniform_Manifold_Approximation_and_Projection" class="mw-redirect" title="Uniform Manifold Approximation and Projection">Uniform Manifold Approximation and Projection</a></div>
<p><a href="/wiki/Uniform_manifold_approximation_and_projection" class="mw-redirect" title="Uniform manifold approximation and projection">Uniform manifold approximation and projection</a> (UMAP) is a nonlinear dimensionality reduction technique. Visually, it is similar to t-SNE, but it assumes that the data is uniformly distributed on a <a href="/wiki/Locally_connected" class="mw-redirect" title="Locally connected">locally connected</a> <a href="/wiki/Riemannian_manifold" title="Riemannian manifold">Riemannian manifold</a> and that the <a href="/wiki/Riemannian_metric" class="mw-redirect" title="Riemannian metric">Riemannian metric</a> is locally constant or approximately locally constant.
</p>
<h2><span class="mw-headline" id="Dimension_reduction">Dimension reduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=12" title="Edit section: Dimension reduction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>For high-dimensional datasets (i.e. with number of dimensions more than 10), dimension reduction is usually performed prior to applying a <a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm">K-nearest neighbors algorithm</a> (k-NN) in order to avoid the effects of the <a href="/wiki/Curse_of_dimensionality" title="Curse of dimensionality">curse of dimensionality</a>.<sup id="cite_ref-19" class="reference"><a href="#cite_note-19">&#91;19&#93;</a></sup>
</p><p><a href="/wiki/Feature_extraction" title="Feature extraction">Feature extraction</a> and  dimension reduction can be combined in one step using <a href="/wiki/Principal_component_analysis" title="Principal component analysis">principal component analysis</a> (PCA),  <a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">linear discriminant analysis</a> (LDA), <a href="/wiki/Canonical_correlation_analysis" class="mw-redirect" title="Canonical correlation analysis">canonical correlation analysis</a> (CCA), or <a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">non-negative matrix factorization</a> (NMF) techniques as a pre-processing step followed by clustering by K-NN on <a href="/wiki/Feature_(machine_learning)" title="Feature (machine learning)">feature vectors</a> in reduced-dimension space. In <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> this process is also called low-dimensional <a href="/wiki/Embedding" title="Embedding">embedding</a>.<sup id="cite_ref-20" class="reference"><a href="#cite_note-20">&#91;20&#93;</a></sup>
</p><p>For very-high-dimensional datasets (e.g. when performing similarity search on live video streams, DNA data or high-dimensional <a href="/wiki/Time_series" title="Time series">time series</a>) running a fast <b>approximate</b> K-NN search using <a href="/wiki/Locality_sensitive_hashing" class="mw-redirect" title="Locality sensitive hashing">locality sensitive hashing</a>, <a href="/wiki/Random_projection" title="Random projection">random projection</a>,<sup id="cite_ref-21" class="reference"><a href="#cite_note-21">&#91;21&#93;</a></sup> "sketches" <sup id="cite_ref-22" class="reference"><a href="#cite_note-22">&#91;22&#93;</a></sup> or other high-dimensional similarity search  techniques from the <a href="/wiki/VLDB_conference" class="mw-redirect" title="VLDB conference">VLDB</a> toolbox might be the only feasible option.
</p>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=13" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>A dimensionality reduction technique that is sometimes used in <a href="/wiki/Neuroscience" title="Neuroscience">neuroscience</a> is <a href="/wiki/Maximally_informative_dimensions" title="Maximally informative dimensions">maximally informative dimensions</a>,<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (June 2017)">citation needed</span></a></i>&#93;</sup> which finds a lower-dimensional representation of a dataset such that as much <a href="/wiki/Mutual_information" title="Mutual information">information</a> as possible about the original data is preserved.
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=14" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:18.0em;margin:0 0 1.0em 1.0em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="/wiki/Recommender_system" title="Recommender system">Recommender systems</a></th></tr><tr><th style="padding:0.1em;border-top:1px solid #aaa;">
Concepts</th></tr><tr><td class="hlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/Collective_intelligence" title="Collective intelligence">Collective intelligence</a></li>
<li><a href="/wiki/Relevance" title="Relevance">Relevance</a></li>
<li><a href="/wiki/Star_(classification)" title="Star (classification)">Star ratings</a></li>
<li><a href="/wiki/Long_tail" title="Long tail">Long tail</a></li></ul></td>
</tr><tr><th style="padding:0.1em;border-top:1px solid #aaa;">
Methods and challenges</th></tr><tr><td class="hlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/Cold_start_(recommender_systems)" title="Cold start (recommender systems)">Cold start</a></li>
<li><a href="/wiki/Collaborative_filtering" title="Collaborative filtering">Collaborative filtering</a></li>
<li><a class="mw-selflink selflink">Dimensionality reduction</a></li>
<li><a href="/wiki/Implicit_data_collection" title="Implicit data collection">Implicit data collection</a></li>
<li><a href="/wiki/Item-item_collaborative_filtering" title="Item-item collaborative filtering">Item-item collaborative filtering</a></li>
<li><a href="/wiki/Matrix_factorization_(recommender_systems)" title="Matrix factorization (recommender systems)">Matrix factorization</a></li>
<li><a href="/wiki/Preference_elicitation" title="Preference elicitation">Preference elicitation</a></li>
<li><a href="/wiki/Similarity_search" title="Similarity search">Similarity search</a></li></ul></td>
</tr><tr><th style="padding:0.1em;border-top:1px solid #aaa;">
Implementations</th></tr><tr><td class="hlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/Collaborative_search_engine" title="Collaborative search engine">Collaborative search engine</a></li>
<li><a href="/wiki/Content_discovery_platform" title="Content discovery platform">Content discovery platform</a></li>
<li><a href="/wiki/Decision_support_system" title="Decision support system">Decision support system</a></li>
<li><a href="/wiki/Music_Genome_Project" title="Music Genome Project">Music Genome Project</a></li>
<li><a href="/wiki/Product_finder" title="Product finder">Product finder</a></li></ul></td>
</tr><tr><th style="padding:0.1em;border-top:1px solid #aaa;">
Research</th></tr><tr><td class="hlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/GroupLens_Research" title="GroupLens Research">GroupLens Research</a></li>
<li><a href="/wiki/MovieLens" title="MovieLens">MovieLens</a></li>
<li><a href="/wiki/Netflix_Prize" title="Netflix Prize">Netflix Prize</a></li></ul></td>
</tr><tr><td style="text-align:right;font-size:115%"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Recommender_systems" title="Template:Recommender systems"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Recommender_systems" title="Template talk:Recommender systems"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Recommender_systems&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<div class="div-col columns column-width" style="-moz-column-width:&#32;; -webkit-column-width:&#32;; column-width:&#32;;">
<ul><li><a href="/wiki/Nearest_neighbor_search" title="Nearest neighbor search">Nearest neighbor search</a></li>
<li><a href="/wiki/MinHash" title="MinHash">MinHash</a></li>
<li><a href="/wiki/Information_gain_in_decision_trees" title="Information gain in decision trees">Information gain in decision trees</a></li>
<li><a href="/wiki/Semidefinite_embedding" title="Semidefinite embedding">Semidefinite embedding</a></li>
<li><a href="/wiki/Multifactor_dimensionality_reduction" title="Multifactor dimensionality reduction">Multifactor dimensionality reduction</a></li>
<li><a href="/wiki/Multilinear_subspace_learning" title="Multilinear subspace learning">Multilinear subspace learning</a></li>
<li><a href="/wiki/Multilinear_PCA" class="mw-redirect" title="Multilinear PCA">Multilinear PCA</a></li>
<li><a href="/wiki/Random_projection" title="Random projection">Random projection</a></li>
<li><a href="/wiki/Singular_value_decomposition" title="Singular value decomposition">Singular value decomposition</a></li>
<li><a href="/wiki/Latent_semantic_analysis" title="Latent semantic analysis">Latent semantic analysis</a></li>
<li><a href="/wiki/Semantic_mapping_(statistics)" title="Semantic mapping (statistics)">Semantic mapping</a></li>
<li><a href="/wiki/Topological_data_analysis" title="Topological data analysis">Topological data analysis</a></li>
<li><a href="/wiki/Locality_sensitive_hashing" class="mw-redirect" title="Locality sensitive hashing">Locality sensitive hashing</a></li>
<li><a href="/wiki/Sufficient_dimension_reduction" title="Sufficient dimension reduction">Sufficient dimension reduction</a></li>
<li><a href="/wiki/Data_transformation_(statistics)" title="Data transformation (statistics)">Data transformation (statistics)</a></li>
<li><a href="/wiki/Weighted_correlation_network_analysis" title="Weighted correlation network analysis">Weighted correlation network analysis</a></li>
<li><a href="/wiki/Hyperparameter_optimization" title="Hyperparameter optimization">Hyperparameter optimization</a></li>
<li><a href="/wiki/CUR_matrix_approximation" title="CUR matrix approximation">CUR matrix approximation</a></li>
<li>Envelope model</li>
<li><a href="/wiki/Nonlinear_dimensionality_reduction" title="Nonlinear dimensionality reduction">Nonlinear dimensionality reduction</a></li>
<li><a href="/wiki/Sammon_mapping" title="Sammon mapping">Sammon mapping</a></li>
<li><a href="/wiki/Johnson%E2%80%93Lindenstrauss_lemma" title="Johnson–Lindenstrauss lemma">Johnson–Lindenstrauss lemma</a></li>
<li><a href="/wiki/Local_tangent_space_alignment" title="Local tangent space alignment">Local tangent space alignment</a></li></ul></div>
<h2><span class="mw-headline" id="Notes">Notes</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=15" title="Edit section: Notes">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist" style="list-style-type: decimal;">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-dr_review-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-dr_review_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-dr_review_1-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite id="CITEREFvan_der_MaatenPostmavan_den_Herik2009" class="citation journal">van der Maaten, Laurens; Postma, Eric; van den Herik, Jaap (October 26, 2009). <a rel="nofollow" class="external text" href="https://members.loria.fr/moberger/Enseignement/AVR/Exposes/TR_Dimensiereductie.pdf">"Dimensionality Reduction: A Comparative Review"</a> <span class="cs1-format">(PDF)</span>. <i>J Mach Learn Res</i>. <b>10</b>: 66–71.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=J+Mach+Learn+Res&amp;rft.atitle=Dimensionality+Reduction%3A+A+Comparative+Review&amp;rft.volume=10&amp;rft.pages=66-71&amp;rft.date=2009-10-26&amp;rft.aulast=van+der+Maaten&amp;rft.aufirst=Laurens&amp;rft.au=Postma%2C+Eric&amp;rft.au=van+den+Herik%2C+Jaap&amp;rft_id=https%3A%2F%2Fmembers.loria.fr%2Fmoberger%2FEnseignement%2FAVR%2FExposes%2FTR_Dimensiereductie.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><style data-mw-deduplicate="TemplateStyles:r951705291">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background-image:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png");background-image:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg");background-repeat:no-repeat;background-size:9px;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background-image:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png");background-image:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg");background-repeat:no-repeat;background-size:9px;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background-image:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png");background-image:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg");background-repeat:no-repeat;background-size:9px;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background-image:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png");background-image:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg");background-repeat:no-repeat;background-size:12px;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><cite id="CITEREFPudilNovovičová1998" class="citation book">Pudil, P.; Novovičová, J. (1998). "Novel Methods for Feature Subset Selection with Respect to Problem Knowledge".  In Liu, Huan; Motoda, Hiroshi (eds.). <i>Feature Extraction, Construction and Selection</i>. p.&#160;101. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2F978-1-4615-5725-8_7">10.1007/978-1-4615-5725-8_7</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-4613-7622-4" title="Special:BookSources/978-1-4613-7622-4"><bdi>978-1-4613-7622-4</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Novel+Methods+for+Feature+Subset+Selection+with+Respect+to+Problem+Knowledge&amp;rft.btitle=Feature+Extraction%2C+Construction+and+Selection&amp;rft.pages=101&amp;rft.date=1998&amp;rft_id=info%3Adoi%2F10.1007%2F978-1-4615-5725-8_7&amp;rft.isbn=978-1-4613-7622-4&amp;rft.aulast=Pudil&amp;rft.aufirst=P.&amp;rft.au=Novovi%C4%8Dov%C3%A1%2C+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><cite id="CITEREFRico-Sulayes2017" class="citation journal">Rico-Sulayes, Antonio (2017). <a rel="nofollow" class="external text" href="http://rielac.cujae.edu.cu/index.php/rieac/article/download/478/278">"Reducing Vector Space Dimensionality in Automatic Classification for Authorship Attribution"</a>. <i>Revista Ingeniería Electrónica, Automática y Comunicaciones</i>. <b>38</b> (3): 26–35.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Revista+Ingenier%C3%ADa+Electr%C3%B3nica%2C+Autom%C3%A1tica+y+Comunicaciones&amp;rft.atitle=Reducing+Vector+Space+Dimensionality+in+Automatic+Classification+for+Authorship+Attribution&amp;rft.volume=38&amp;rft.issue=3&amp;rft.pages=26-35&amp;rft.date=2017&amp;rft.aulast=Rico-Sulayes&amp;rft.aufirst=Antonio&amp;rft_id=http%3A%2F%2Frielac.cujae.edu.cu%2Findex.php%2Frieac%2Farticle%2Fdownload%2F478%2F278&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text">Samet, H. (2006) <i>Foundations of Multidimensional and Metric Data Structures</i>. Morgan Kaufmann. <link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/><a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-12-369446-9" title="Special:BookSources/0-12-369446-9">0-12-369446-9</a></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text">C. Ding, X. He, H. Zha, H.D. Simon, <a rel="nofollow" class="external text" href="https://cloudfront.escholarship.org/dist/prd/content/qt8pv153t1/qt8pv153t1.pdf">Adaptive Dimension Reduction for Clustering High Dimensional Data</a>, Proceedings of International Conference on Data Mining, 2002</span>
</li>
<li id="cite_note-MSLsurvey-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-MSLsurvey_6-0">^</a></b></span> <span class="reference-text"><cite id="CITEREFLuPlataniotisVenetsanopoulos2011" class="citation journal">Lu, Haiping; Plataniotis, K.N.; Venetsanopoulos, A.N. (2011). <a rel="nofollow" class="external text" href="http://www.dsp.utoronto.ca/~haiping/Publication/SurveyMSL_PR2011.pdf">"A Survey of Multilinear Subspace Learning for Tensor Data"</a> <span class="cs1-format">(PDF)</span>. <i>Pattern Recognition</i>. <b>44</b> (7): 1540–1551. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.patcog.2011.01.004">10.1016/j.patcog.2011.01.004</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Pattern+Recognition&amp;rft.atitle=A+Survey+of+Multilinear+Subspace+Learning+for+Tensor+Data&amp;rft.volume=44&amp;rft.issue=7&amp;rft.pages=1540-1551&amp;rft.date=2011&amp;rft_id=info%3Adoi%2F10.1016%2Fj.patcog.2011.01.004&amp;rft.aulast=Lu&amp;rft.aufirst=Haiping&amp;rft.au=Plataniotis%2C+K.N.&amp;rft.au=Venetsanopoulos%2C+A.N.&amp;rft_id=http%3A%2F%2Fwww.dsp.utoronto.ca%2F~haiping%2FPublication%2FSurveyMSL_PR2011.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-lee-seung-7"><span class="mw-cite-backlink">^ <a href="#cite_ref-lee-seung_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-lee-seung_7-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite id="CITEREFDaniel_D._LeeH._Sebastian_Seung1999" class="citation journal">Daniel D. Lee &amp; <a href="/wiki/Sebastian_Seung" title="Sebastian Seung">H. Sebastian Seung</a> (1999). "Learning the parts of objects by non-negative matrix factorization". <i><a href="/wiki/Nature_(journal)" title="Nature (journal)">Nature</a></i>. <b>401</b> (6755): 788–791. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/1999Natur.401..788L">1999Natur.401..788L</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2F44565">10.1038/44565</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/10548103">10548103</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Learning+the+parts+of+objects+by+non-negative+matrix+factorization&amp;rft.volume=401&amp;rft.issue=6755&amp;rft.pages=788-791&amp;rft.date=1999&amp;rft_id=info%3Apmid%2F10548103&amp;rft_id=info%3Adoi%2F10.1038%2F44565&amp;rft_id=info%3Abibcode%2F1999Natur.401..788L&amp;rft.au=Daniel+D.+Lee&amp;rft.au=H.+Sebastian+Seung&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-lee2001algorithms-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-lee2001algorithms_8-0">^</a></b></span> <span class="reference-text"><cite id="CITEREFDaniel_D._LeeH._Sebastian_Seung2001" class="citation conference">Daniel D. Lee &amp; H. Sebastian Seung (2001). <a rel="nofollow" class="external text" href="http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf"><i>Algorithms for Non-negative Matrix Factorization</i></a> <span class="cs1-format">(PDF)</span>. Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference. <a href="/wiki/MIT_Press" title="MIT Press">MIT Press</a>. pp.&#160;556–562.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Algorithms+for+Non-negative+Matrix+Factorization&amp;rft.pages=556-562&amp;rft.pub=MIT+Press&amp;rft.date=2001&amp;rft.au=Daniel+D.+Lee&amp;rft.au=H.+Sebastian+Seung&amp;rft_id=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F1861-algorithms-for-non-negative-matrix-factorization.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-blantonRoweis07-9"><span class="mw-cite-backlink">^ <a href="#cite_ref-blantonRoweis07_9-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-blantonRoweis07_9-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite id="CITEREFBlantonRoweis2007" class="citation journal">Blanton, Michael R.; Roweis, Sam (2007). "K-corrections and filter transformations in the ultraviolet, optical, and near infrared". <i>The Astronomical Journal</i>. <b>133</b> (2): 734–754. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/astro-ph/0606170">astro-ph/0606170</a></span>. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2007AJ....133..734B">2007AJ....133..734B</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1086%2F510127">10.1086/510127</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Astronomical+Journal&amp;rft.atitle=K-corrections+and+filter+transformations+in+the+ultraviolet%2C+optical%2C+and+near+infrared&amp;rft.volume=133&amp;rft.issue=2&amp;rft.pages=734-754&amp;rft.date=2007&amp;rft_id=info%3Aarxiv%2Fastro-ph%2F0606170&amp;rft_id=info%3Adoi%2F10.1086%2F510127&amp;rft_id=info%3Abibcode%2F2007AJ....133..734B&amp;rft.aulast=Blanton&amp;rft.aufirst=Michael+R.&amp;rft.au=Roweis%2C+Sam&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-ren18-10"><span class="mw-cite-backlink">^ <a href="#cite_ref-ren18_10-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ren18_10-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-ren18_10-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-ren18_10-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><cite id="CITEREFRenPueyoZhuDuchêne2018" class="citation journal">Ren, Bin; Pueyo, Laurent; Zhu, Guangtun B.; Duchêne, Gaspard (2018). "Non-negative Matrix Factorization: Robust Extraction of Extended Structures". <i>The Astrophysical Journal</i>. <b>852</b> (2): 104. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1712.10317">1712.10317</a></span>. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2018ApJ...852..104R">2018ApJ...852..104R</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.3847%2F1538-4357%2Faaa1f2">10.3847/1538-4357/aaa1f2</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Astrophysical+Journal&amp;rft.atitle=Non-negative+Matrix+Factorization%3A+Robust+Extraction+of+Extended+Structures&amp;rft.volume=852&amp;rft.issue=2&amp;rft.pages=104&amp;rft.date=2018&amp;rft_id=info%3Aarxiv%2F1712.10317&amp;rft_id=info%3Adoi%2F10.3847%2F1538-4357%2Faaa1f2&amp;rft_id=info%3Abibcode%2F2018ApJ...852..104R&amp;rft.aulast=Ren&amp;rft.aufirst=Bin&amp;rft.au=Pueyo%2C+Laurent&amp;rft.au=Zhu%2C+Guangtun+B.&amp;rft.au=Duch%C3%AAne%2C+Gaspard&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-zhu16-11"><span class="mw-cite-backlink">^ <a href="#cite_ref-zhu16_11-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-zhu16_11-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-zhu16_11-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"> <cite id="CITEREFZhu2016" class="citation arxiv">Zhu, Guangtun B. (2016-12-19). "Nonnegative Matrix Factorization (NMF) with Heteroscedastic Uncertainties and Missing data". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1612.06037">1612.06037</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/astro-ph.IM">astro-ph.IM</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Nonnegative+Matrix+Factorization+%28NMF%29+with+Heteroscedastic+Uncertainties+and+Missing+data&amp;rft.date=2016-12-19&amp;rft_id=info%3Aarxiv%2F1612.06037&amp;rft.aulast=Zhu&amp;rft.aufirst=Guangtun+B.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-ren20”-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-ren20”_12-0">^</a></b></span> <span class="reference-text"><cite id="CITEREFRenPueyoChenChoquet2020" class="citation journal">Ren, Bin; Pueyo, Laurent; Chen, Christine; Choquet, Elodie; Debes, John H.; Duechene, Gaspard; Menard, Francois; Perrin, Marshall D. (2020). "Using Data Imputation for Signal Separation in High Contrast Imaging". <i>The Astrophysical Journal</i>. <b>892</b> (2): 74. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/2001.00563">2001.00563</a></span>. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2020ApJ...892...74R">2020ApJ...892...74R</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.3847%2F1538-4357%2Fab7024">10.3847/1538-4357/ab7024</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Astrophysical+Journal&amp;rft.atitle=Using+Data+Imputation+for+Signal+Separation+in+High+Contrast+Imaging&amp;rft.volume=892&amp;rft.issue=2&amp;rft.pages=74&amp;rft.date=2020&amp;rft_id=info%3Aarxiv%2F2001.00563&amp;rft_id=info%3Adoi%2F10.3847%2F1538-4357%2Fab7024&amp;rft_id=info%3Abibcode%2F2020ApJ...892...74R&amp;rft.aulast=Ren&amp;rft.aufirst=Bin&amp;rft.au=Pueyo%2C+Laurent&amp;rft.au=Chen%2C+Christine&amp;rft.au=Choquet%2C+Elodie&amp;rft.au=Debes%2C+John+H.&amp;rft.au=Duechene%2C+Gaspard&amp;rft.au=Menard%2C+Francois&amp;rft.au=Perrin%2C+Marshall+D.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><cite id="CITEREFRoweisSaul2000" class="citation journal">Roweis, S. T.; Saul, L. K. (2000). "Nonlinear Dimensionality Reduction by Locally Linear Embedding". <i>Science</i>. <b>290</b> (5500): 2323–2326. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2000Sci...290.2323R">2000Sci...290.2323R</a>. <a href="/wiki/CiteSeerX_(identifier)" class="mw-redirect" title="CiteSeerX (identifier)">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.111.3313">10.1.1.111.3313</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1126%2Fscience.290.5500.2323">10.1126/science.290.5500.2323</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/11125150">11125150</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=Nonlinear+Dimensionality+Reduction+by+Locally+Linear+Embedding&amp;rft.volume=290&amp;rft.issue=5500&amp;rft.pages=2323-2326&amp;rft.date=2000&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.111.3313&amp;rft_id=info%3Apmid%2F11125150&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.290.5500.2323&amp;rft_id=info%3Abibcode%2F2000Sci...290.2323R&amp;rft.aulast=Roweis&amp;rft.aufirst=S.+T.&amp;rft.au=Saul%2C+L.+K.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><cite id="CITEREFZhangZha2004" class="citation journal">Zhang, Zhenyue; Zha, Hongyuan (2004). "Principal Manifolds and Nonlinear Dimensionality Reduction via Tangent Space Alignment". <i>SIAM Journal on Scientific Computing</i>. <b>26</b> (1): 313–338. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1137%2Fs1064827502419154">10.1137/s1064827502419154</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=SIAM+Journal+on+Scientific+Computing&amp;rft.atitle=Principal+Manifolds+and+Nonlinear+Dimensionality+Reduction+via+Tangent+Space+Alignment&amp;rft.volume=26&amp;rft.issue=1&amp;rft.pages=313-338&amp;rft.date=2004&amp;rft_id=info%3Adoi%2F10.1137%2Fs1064827502419154&amp;rft.aulast=Zhang&amp;rft.aufirst=Zhenyue&amp;rft.au=Zha%2C+Hongyuan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><cite id="CITEREFBengioMonperrusLarochelle2006" class="citation journal">Bengio, Yoshua; Monperrus, Martin; Larochelle, Hugo (2006). <a rel="nofollow" class="external text" href="https://hal.archives-ouvertes.fr/hal-01575345/document">"Nonlocal Estimation of Manifold Structure"</a>. <i>Neural Computation</i>. <b>18</b> (10): 2509–2528. <a href="/wiki/CiteSeerX_(identifier)" class="mw-redirect" title="CiteSeerX (identifier)">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.116.4230">10.1.1.116.4230</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1162%2Fneco.2006.18.10.2509">10.1162/neco.2006.18.10.2509</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/16907635">16907635</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computation&amp;rft.atitle=Nonlocal+Estimation+of+Manifold+Structure&amp;rft.volume=18&amp;rft.issue=10&amp;rft.pages=2509-2528&amp;rft.date=2006&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.116.4230&amp;rft_id=info%3Apmid%2F16907635&amp;rft_id=info%3Adoi%2F10.1162%2Fneco.2006.18.10.2509&amp;rft.aulast=Bengio&amp;rft.aufirst=Yoshua&amp;rft.au=Monperrus%2C+Martin&amp;rft.au=Larochelle%2C+Hugo&amp;rft_id=https%3A%2F%2Fhal.archives-ouvertes.fr%2Fhal-01575345%2Fdocument&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text">Hongbing Hu, Stephen A. Zahorian, (2010) <a rel="nofollow" class="external text" href="http://bingweb.binghamton.edu/~hhu1/paper/Hu2010Dimensionality.pdf">"Dimensionality Reduction Methods for HMM Phonetic Recognition,"</a> ICASSP 2010, Dallas, TX</span>
</li>
<li id="cite_note-gda-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-gda_17-0">^</a></b></span> <span class="reference-text"><cite id="CITEREFBaudatAnouar2000" class="citation journal">Baudat, G.; Anouar, F. (2000). "Generalized Discriminant Analysis Using a Kernel Approach". <i>Neural Computation</i>. <b>12</b> (10): 2385–2404. <a href="/wiki/CiteSeerX_(identifier)" class="mw-redirect" title="CiteSeerX (identifier)">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.412.760">10.1.1.412.760</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1162%2F089976600300014980">10.1162/089976600300014980</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/11032039">11032039</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computation&amp;rft.atitle=Generalized+Discriminant+Analysis+Using+a+Kernel+Approach&amp;rft.volume=12&amp;rft.issue=10&amp;rft.pages=2385-2404&amp;rft.date=2000&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.412.760&amp;rft_id=info%3Apmid%2F11032039&amp;rft_id=info%3Adoi%2F10.1162%2F089976600300014980&amp;rft.aulast=Baudat&amp;rft.aufirst=G.&amp;rft.au=Anouar%2C+F.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-cloudid-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-cloudid_18-0">^</a></b></span> <span class="reference-text"><cite id="CITEREFHaghighatZonouzAbdel-Mottaleb2015" class="citation journal">Haghighat, Mohammad; Zonouz, Saman; Abdel-Mottaleb, Mohamed (2015). "CloudID: Trustworthy cloud-based and cross-enterprise biometric identification". <i>Expert Systems with Applications</i>. <b>42</b> (21): 7905–7916. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.eswa.2015.06.025">10.1016/j.eswa.2015.06.025</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Expert+Systems+with+Applications&amp;rft.atitle=CloudID%3A+Trustworthy+cloud-based+and+cross-enterprise+biometric+identification&amp;rft.volume=42&amp;rft.issue=21&amp;rft.pages=7905-7916&amp;rft.date=2015&amp;rft_id=info%3Adoi%2F10.1016%2Fj.eswa.2015.06.025&amp;rft.aulast=Haghighat&amp;rft.aufirst=Mohammad&amp;rft.au=Zonouz%2C+Saman&amp;rft.au=Abdel-Mottaleb%2C+Mohamed&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text">Kevin Beyer, Jonathan Goldstein, Raghu Ramakrishnan, Uri Shaft (1999) <a rel="nofollow" class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.31.1422">"When is “nearest neighbor” meaningful?"</a>. <i>Database Theory—ICDT99</i>,  217–235</span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><cite id="CITEREFShawJebara2009" class="citation book">Shaw, B.; Jebara, T. (2009). <a rel="nofollow" class="external text" href="https://www.cs.columbia.edu/~jebara/papers/spe-icml09.pdf">"Structure preserving embedding"</a> <span class="cs1-format">(PDF)</span>. <i>Proceedings of the 26th Annual International Conference on Machine Learning – ICML '09</i>. p.&#160;1. <a href="/wiki/CiteSeerX_(identifier)" class="mw-redirect" title="CiteSeerX (identifier)">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.161.451">10.1.1.161.451</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F1553374.1553494">10.1145/1553374.1553494</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781605585161" title="Special:BookSources/9781605585161"><bdi>9781605585161</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Structure+preserving+embedding&amp;rft.btitle=Proceedings+of+the+26th+Annual+International+Conference+on+Machine+Learning+%E2%80%93+ICML+%2709&amp;rft.pages=1&amp;rft.date=2009&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.161.451&amp;rft_id=info%3Adoi%2F10.1145%2F1553374.1553494&amp;rft.isbn=9781605585161&amp;rft.aulast=Shaw&amp;rft.aufirst=B.&amp;rft.au=Jebara%2C+T.&amp;rft_id=https%3A%2F%2Fwww.cs.columbia.edu%2F~jebara%2Fpapers%2Fspe-icml09.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><cite id="CITEREFBinghamMannila2001" class="citation book">Bingham, E.; Mannila, H. (2001). "Random projection in dimensionality reduction". <i>Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining – KDD '01</i>. p.&#160;245. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F502512.502546">10.1145/502512.502546</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1581133912" title="Special:BookSources/978-1581133912"><bdi>978-1581133912</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Random+projection+in+dimensionality+reduction&amp;rft.btitle=Proceedings+of+the+seventh+ACM+SIGKDD+international+conference+on+Knowledge+discovery+and+data+mining+%E2%80%93+KDD+%2701&amp;rft.pages=245&amp;rft.date=2001&amp;rft_id=info%3Adoi%2F10.1145%2F502512.502546&amp;rft.isbn=978-1581133912&amp;rft.aulast=Bingham&amp;rft.aufirst=E.&amp;rft.au=Mannila%2C+H.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text">Shasha, D High (2004) <i>Performance Discovery in Time Series</i> Berlin: Springer. <link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/><a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-387-00857-8" title="Special:BookSources/0-387-00857-8">0-387-00857-8</a></span>
</li>
</ol></div></div>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=16" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<style data-mw-deduplicate="TemplateStyles:r886047268">.mw-parser-output .refbegin{font-size:90%;margin-bottom:0.5em}.mw-parser-output .refbegin-hanging-indents>ul{list-style-type:none;margin-left:0}.mw-parser-output .refbegin-hanging-indents>ul>li,.mw-parser-output .refbegin-hanging-indents>dl>dd{margin-left:0;padding-left:3.2em;text-indent:-3.2em;list-style:none}.mw-parser-output .refbegin-100{font-size:100%}</style><div class="refbegin reflist" style="">
<ul><li><cite id="CITEREFBoehmkeGreenwell2019" class="citation book">Boehmke, Brad; Greenwell, Brandon M. (2019). <a rel="nofollow" class="external text" href="https://books.google.com/books?id=aXC9DwAAQBAJ&amp;pg=PA343">"Dimension Reduction"</a>. <i>Hands-On Machine Learning with R</i>. Chapman &amp; Hall. pp.&#160;343–396. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-138-49568-5" title="Special:BookSources/978-1-138-49568-5"><bdi>978-1-138-49568-5</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Dimension+Reduction&amp;rft.btitle=Hands-On+Machine+Learning+with+R&amp;rft.pages=343-396&amp;rft.pub=Chapman+%26+Hall&amp;rft.date=2019&amp;rft.isbn=978-1-138-49568-5&amp;rft.aulast=Boehmke&amp;rft.aufirst=Brad&amp;rft.au=Greenwell%2C+Brandon+M.&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DaXC9DwAAQBAJ%26pg%3DPA343&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></li>
<li><cite id="CITEREFFodor2002" class="citation techreport">Fodor, I. (2002). <a rel="nofollow" class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/versions?doi=10.1.1.8.5098"><i>A survey of dimension reduction techniques</i></a> (Technical report). Center for Applied Scientific Computing, Lawrence Livermore National. UCRL-ID-148494.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=A+survey+of+dimension+reduction+techniques&amp;rft.pub=Center+for+Applied+Scientific+Computing%2C+Lawrence+Livermore+National&amp;rft.date=2002&amp;rft.aulast=Fodor&amp;rft.aufirst=I.&amp;rft_id=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fversions%3Fdoi%3D10.1.1.8.5098&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></li>
<li><cite id="CITEREFCunningham2007" class="citation techreport">Cunningham, P. (2007). <a rel="nofollow" class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.98.1478"><i>Dimension Reduction</i></a> (Technical report). University College Dublin. UCD-CSI-2007-7.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=Dimension+Reduction&amp;rft.pub=University+College+Dublin&amp;rft.date=2007&amp;rft.aulast=Cunningham&amp;rft.aufirst=P.&amp;rft_id=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.98.1478&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></li>
<li><cite id="CITEREFLakshmi_PadmajaVishnuvardhan2016" class="citation book">Lakshmi Padmaja, Dhyaram; Vishnuvardhan, B (2016). "Comparative Study of Feature Subset Selection Methods for Dimensionality Reduction on Scientific Data". <i>2016 IEEE 6th International Conference on Advanced Computing (IACC)</i>. pp.&#160;31–34. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FIACC.2016.16">10.1109/IACC.2016.16</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-4673-8286-1" title="Special:BookSources/978-1-4673-8286-1"><bdi>978-1-4673-8286-1</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Comparative+Study+of+Feature+Subset+Selection+Methods+for+Dimensionality+Reduction+on+Scientific+Data&amp;rft.btitle=2016+IEEE+6th+International+Conference+on+Advanced+Computing+%28IACC%29&amp;rft.pages=31-34&amp;rft.date=2016&amp;rft_id=info%3Adoi%2F10.1109%2FIACC.2016.16&amp;rft.isbn=978-1-4673-8286-1&amp;rft.aulast=Lakshmi+Padmaja&amp;rft.aufirst=Dhyaram&amp;rft.au=Vishnuvardhan%2C+B&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></li></ul>
</div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=17" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a rel="nofollow" class="external text" href="http://jmlr.csail.mit.edu/papers/special/feature03.html">JMLR Special Issue on Variable and Feature Selection</a></li>
<li><a rel="nofollow" class="external text" href="http://bioinfo-out.curie.fr/projects/elmap/">ELastic MAPs</a></li>
<li><a rel="nofollow" class="external text" href="http://www.cs.toronto.edu/~roweis/lle">Locally Linear Embedding</a></li>
<li><a rel="nofollow" class="external text" href="https://web.archive.org/web/20040411051530/http://isomap.stanford.edu/">A Global Geometric Framework for Nonlinear Dimensionality Reduction</a></li></ul>
<!-- 
NewPP limit report
Parsed by mw1406
Cached time: 20200703162224
Cache expiry: 2592000
Dynamic content: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.408 seconds
Real time usage: 0.537 seconds
Preprocessor visited node count: 2443/1000000
Post‐expand include size: 63061/2097152 bytes
Template argument size: 3072/2097152 bytes
Highest expansion depth: 16/40
Expensive parser function count: 3/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 94312/5000000 bytes
Lua time usage: 0.198/10.000 seconds
Lua memory usage: 5.89 MB/50 MB
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  478.919      1 -total
 48.48%  232.157      1 Template:Reflist
 27.82%  133.255     12 Template:Cite_journal
 12.78%   61.182      1 Template:Short_description
 12.15%   58.193      2 Template:Citation_needed
 11.95%   57.223      3 Template:Fix
  9.72%   46.568      1 Template:Pagetype
  8.84%   42.360      2 Template:ISBN
  6.60%   31.592      5 Template:Category_handler
  5.41%   25.895      5 Template:Cite_book
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:579867-0!canonical and timestamp 20200703162223 and revision id 965821499
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>
		<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Dimensionality_reduction&amp;oldid=965821499">https://en.wikipedia.org/w/index.php?title=Dimensionality_reduction&amp;oldid=965821499</a>"</div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Dimension_reduction" title="Category:Dimension reduction">Dimension reduction</a></li><li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_September_2017" title="Category:Articles with unsourced statements from September 2017">Articles with unsourced statements from September 2017</a></li><li><a href="/wiki/Category:Wikipedia_articles_needing_clarification_from_September_2017" title="Category:Wikipedia articles needing clarification from September 2017">Wikipedia articles needing clarification from September 2017</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_June_2017" title="Category:Articles with unsourced statements from June 2017">Articles with unsourced statements from June 2017</a></li></ul></div></div>
		<div class="visualClear"></div>
		
	</div>
</div>
<div id='mw-data-after-content'>
	<div class="read-more-container"></div>
</div>

<div id="mw-navigation">
	<h2>Navigation menu</h2>
	<div id="mw-head">
		<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-personal" class="vector-menu" aria-labelledby="p-personal-label" role="navigation" 
	 >
	<h3 id="p-personal-label">
		<span>Personal tools</span>
	</h3>
	<!-- Please do not use the .body class, it is deprecated. -->
	<div class="body vector-menu-content">
		<!-- Please do not use the .menu class, it is deprecated. -->
		<ul class="vector-menu-content-list"><li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Dimensionality+reduction" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Dimensionality+reduction" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li></ul>
		
	</div>
</nav>


		<div id="left-navigation">
			<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-namespaces" class="vector-menu vector-menu-tabs vectorTabs" aria-labelledby="p-namespaces-label" role="navigation" 
	 >
	<h3 id="p-namespaces-label">
		<span>Namespaces</span>
	</h3>
	<!-- Please do not use the .body class, it is deprecated. -->
	<div class="body vector-menu-content">
		<!-- Please do not use the .menu class, it is deprecated. -->
		<ul class="vector-menu-content-list"><li id="ca-nstab-main" class="selected"><a href="/wiki/Dimensionality_reduction" title="View the content page [c]" accesskey="c">Article</a></li><li id="ca-talk"><a href="/wiki/Talk:Dimensionality_reduction" rel="discussion" title="Discuss improvements to the content page [t]" accesskey="t">Talk</a></li></ul>
		
	</div>
</nav>


			<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-variants" class="vector-menu-empty emptyPortlet vector-menu vector-menu-dropdown vectorMenu" aria-labelledby="p-variants-label" role="navigation" 
	 >
	<input type="checkbox" class="vector-menu-checkbox vectorMenuCheckbox" aria-labelledby="p-variants-label" />
	<h3 id="p-variants-label">
		<span>Variants</span>
	</h3>
	<!-- Please do not use the .body class, it is deprecated. -->
	<div class="body vector-menu-content">
		<!-- Please do not use the .menu class, it is deprecated. -->
		<ul class="menu vector-menu-content-list"></ul>
		
	</div>
</nav>


		</div>
		<div id="right-navigation">
			<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-views" class="vector-menu vector-menu-tabs vectorTabs" aria-labelledby="p-views-label" role="navigation" 
	 >
	<h3 id="p-views-label">
		<span>Views</span>
	</h3>
	<!-- Please do not use the .body class, it is deprecated. -->
	<div class="body vector-menu-content">
		<!-- Please do not use the .menu class, it is deprecated. -->
		<ul class="vector-menu-content-list"><li id="ca-view" class="collapsible selected"><a href="/wiki/Dimensionality_reduction">Read</a></li><li id="ca-edit" class="collapsible"><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></li><li id="ca-history" class="collapsible"><a href="/w/index.php?title=Dimensionality_reduction&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></li></ul>
		
	</div>
</nav>


			<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-cactions" class="vector-menu-empty emptyPortlet vector-menu vector-menu-dropdown vectorMenu" aria-labelledby="p-cactions-label" role="navigation" 
	 >
	<input type="checkbox" class="vector-menu-checkbox vectorMenuCheckbox" aria-labelledby="p-cactions-label" />
	<h3 id="p-cactions-label">
		<span>More</span>
	</h3>
	<!-- Please do not use the .body class, it is deprecated. -->
	<div class="body vector-menu-content">
		<!-- Please do not use the .menu class, it is deprecated. -->
		<ul class="menu vector-menu-content-list"></ul>
		
	</div>
</nav>


			<div id="p-search" role="search">
	<h3 >
		<label for="searchInput">Search</label>
	</h3>
	<form action="/w/index.php" id="searchform">
		<div id="simpleSearch">
			<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/>
			<input type="hidden" name="title" value="Special:Search">
			<input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/>
			<input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>
		</div>
	</form>
</div>

		</div>
	</div>
	
<div id="mw-panel">
	<div id="p-logo" role="banner">
		<a  title="Visit the main page" class="mw-wiki-logo" href="/wiki/Main_Page"></a>
	</div>
	<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-navigation" class="vector-menu vector-menu-portal portal portal-first" aria-labelledby="p-navigation-label" role="navigation" 
	 >
	<h3 id="p-navigation-label">
		<span>Navigation</span>
	</h3>
	<!-- Please do not use the .body class, it is deprecated. -->
	<div class="body vector-menu-content">
		<!-- Please do not use the .menu class, it is deprecated. -->
		<ul class="vector-menu-content-list"><li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Visit a randomly selected article [x]" accesskey="x">Random article</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works">About Wikipedia</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact us</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us by donating to the Wikimedia Foundation">Donate</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li></ul>
		
	</div>
</nav>


	<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-interaction" class="vector-menu vector-menu-portal portal" aria-labelledby="p-interaction-label" role="navigation" 
	 >
	<h3 id="p-interaction-label">
		<span>Contribute</span>
	</h3>
	<!-- Please do not use the .body class, it is deprecated. -->
	<div class="body vector-menu-content">
		<!-- Please do not use the .menu class, it is deprecated. -->
		<ul class="vector-menu-content-list"><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]" accesskey="r">Recent changes</a></li><li id="n-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Add images or other media for use on Wikipedia">Upload file</a></li></ul>
		
	</div>
</nav>

<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-tb" class="vector-menu vector-menu-portal portal" aria-labelledby="p-tb-label" role="navigation" 
	 >
	<h3 id="p-tb-label">
		<span>Tools</span>
	</h3>
	<!-- Please do not use the .body class, it is deprecated. -->
	<div class="body vector-menu-content">
		<!-- Please do not use the .menu class, it is deprecated. -->
		<ul class="vector-menu-content-list"><li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Dimensionality_reduction" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Dimensionality_reduction" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Dimensionality_reduction&amp;oldid=965821499" title="Permanent link to this revision of this page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Dimensionality_reduction&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q16000077" title="Structured data on this page hosted by Wikidata [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Dimensionality_reduction&amp;id=965821499&amp;wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li></ul>
		
	</div>
</nav>

<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-coll-print_export" class="vector-menu vector-menu-portal portal" aria-labelledby="p-coll-print_export-label" role="navigation" 
	 >
	<h3 id="p-coll-print_export-label">
		<span>Print/export</span>
	</h3>
	<!-- Please do not use the .body class, it is deprecated. -->
	<div class="body vector-menu-content">
		<!-- Please do not use the .menu class, it is deprecated. -->
		<ul class="vector-menu-content-list"><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Dimensionality+reduction&amp;action=show-download-screen" title="Download this page as a PDF file">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Dimensionality_reduction&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li></ul>
		
	</div>
</nav>


	<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav id="p-lang" class="vector-menu vector-menu-portal portal" aria-labelledby="p-lang-label" role="navigation" 
	 >
	<h3 id="p-lang-label">
		<span>Languages</span>
	</h3>
	<!-- Please do not use the .body class, it is deprecated. -->
	<div class="body vector-menu-content">
		<!-- Please do not use the .menu class, it is deprecated. -->
		<ul class="vector-menu-content-list"><li class="interlanguage-link interwiki-es"><a href="https://es.wikipedia.org/wiki/Reducci%C3%B3n_de_dimensionalidad" title="Reducción de dimensionalidad – Spanish" lang="es" hreflang="es" class="interlanguage-link-target">Español</a></li><li class="interlanguage-link interwiki-fa"><a href="https://fa.wikipedia.org/wiki/%DA%A9%D8%A7%D9%87%D8%B4_%D8%A7%D8%A8%D8%B9%D8%A7%D8%AF" title="کاهش ابعاد – Persian" lang="fa" hreflang="fa" class="interlanguage-link-target">فارسی</a></li><li class="interlanguage-link interwiki-fr"><a href="https://fr.wikipedia.org/wiki/R%C3%A9duction_de_la_dimensionnalit%C3%A9" title="Réduction de la dimensionnalité – French" lang="fr" hreflang="fr" class="interlanguage-link-target">Français</a></li><li class="interlanguage-link interwiki-ko"><a href="https://ko.wikipedia.org/wiki/%EC%B0%A8%EC%9B%90%EC%B6%95%EC%86%8C" title="차원축소 – Korean" lang="ko" hreflang="ko" class="interlanguage-link-target">한국어</a></li><li class="interlanguage-link interwiki-he"><a href="https://he.wikipedia.org/wiki/%D7%94%D7%95%D7%A8%D7%93%D7%AA_%D7%9E%D7%9E%D7%93" title="הורדת ממד – Hebrew" lang="he" hreflang="he" class="interlanguage-link-target">עברית</a></li><li class="interlanguage-link interwiki-ru"><a href="https://ru.wikipedia.org/wiki/%D0%A1%D0%BD%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5_%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D0%B8" title="Снижение размерности – Russian" lang="ru" hreflang="ru" class="interlanguage-link-target">Русский</a></li><li class="interlanguage-link interwiki-uk"><a href="https://uk.wikipedia.org/wiki/%D0%97%D0%BD%D0%B8%D0%B6%D0%B5%D0%BD%D0%BD%D1%8F_%D1%80%D0%BE%D0%B7%D0%BC%D1%96%D1%80%D0%BD%D0%BE%D1%81%D1%82%D1%96" title="Зниження розмірності – Ukrainian" lang="uk" hreflang="uk" class="interlanguage-link-target">Українська</a></li><li class="interlanguage-link interwiki-zh-yue"><a href="https://zh-yue.wikipedia.org/wiki/%E9%99%8D%E7%B6%AD" title="降維 – Cantonese" lang="yue" hreflang="yue" class="interlanguage-link-target">粵語</a></li><li class="interlanguage-link interwiki-zh"><a href="https://zh.wikipedia.org/wiki/%E9%99%8D%E7%BB%B4" title="降维 – Chinese" lang="zh" hreflang="zh" class="interlanguage-link-target">中文</a></li></ul>
		<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q16000077#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
	</div>
</nav>


</div>

</div>

<footer id="footer" class="mw-footer" role="contentinfo" >
	<ul id="footer-info" >
		<li id="footer-info-lastmod"> This page was last edited on 3 July 2020, at 16:22<span class="anonymous-show">&#160;(UTC)</span>.</li>
		<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
	</ul>
	<ul id="footer-places" >
		<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
		<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
		<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
		<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
		<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
		<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
		<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
		<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Dimensionality_reduction&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
	</ul>
	<ul id="footer-icons" class="noprint">
		<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation" loading="lazy" /></a></li>
		<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31" loading="lazy"/></a></li>
	</ul>
	<div style="clear: both;"></div>
</footer>


<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.408","walltime":"0.537","ppvisitednodes":{"value":2443,"limit":1000000},"postexpandincludesize":{"value":63061,"limit":2097152},"templateargumentsize":{"value":3072,"limit":2097152},"expansiondepth":{"value":16,"limit":40},"expensivefunctioncount":{"value":3,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":94312,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  478.919      1 -total"," 48.48%  232.157      1 Template:Reflist"," 27.82%  133.255     12 Template:Cite_journal"," 12.78%   61.182      1 Template:Short_description"," 12.15%   58.193      2 Template:Citation_needed"," 11.95%   57.223      3 Template:Fix","  9.72%   46.568      1 Template:Pagetype","  8.84%   42.360      2 Template:ISBN","  6.60%   31.592      5 Template:Category_handler","  5.41%   25.895      5 Template:Cite_book"]},"scribunto":{"limitreport-timeusage":{"value":"0.198","limit":"10.000"},"limitreport-memusage":{"value":6179867,"limit":52428800}},"cachereport":{"origin":"mw1406","timestamp":"20200703162224","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Dimensionality reduction","url":"https:\/\/en.wikipedia.org\/wiki\/Dimensionality_reduction","sameAs":"http:\/\/www.wikidata.org\/entity\/Q16000077","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q16000077","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2004-04-06T15:10:25Z","dateModified":"2020-07-03T16:22:20Z","headline":"process of reducing the number of random variables under consideration"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":149,"wgHostname":"mw1274"});});</script></body></html>
